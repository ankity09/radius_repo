{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a58cefe5-8dc4-476c-9e2d-150590298c02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Datasets and Labeling Sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7921206-fcc3-45f4-b281-fa4692e7f68b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Introduction\n",
    "This notebook describes how you can:\n",
    "- Create an evaluation set, backed by a Unity Catalog delta table\n",
    "- Leverage subject matter experts (SME) to build an evaluation dataset\n",
    "- Leverage SME to label traces generated by a version of an Agent to understand quality\n",
    "- Give your SME a pre-production version of your Agent so they can chat with the bot and give feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fd25fd6-9945-47af-a474-bc29330cf43c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqq databricks-agents>=0.17.0 databricks-sdk[openai]\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92765cc1-a47b-4382-8343-ca4df2f90b81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Please provide\n",
    "- The destination UC table name for the evaluation dataset\n",
    "- An experiment name to host the labeling sessions\n",
    "- A list of SME emails who can write assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b685faed-f230-49e2-9a7f-eff23bddfbfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_email = spark.sql(\"SELECT current_user() as username\").collect()[0].username\n",
    "# PLEASE CHANGE\n",
    "CATALOG = \"ankit_yadav\"\n",
    "SCHEMA = \"uct_schema\"\n",
    "DATA_TABLE = \"docs_data_texts\"\n",
    "TABLE = \"my_evals\"\n",
    "ASSIGNED_USERS = [\"ankit.yadav@databricks.com\"]\n",
    "EXPERIMENT_NAME = f\"/Users/{user_email}/review_app_notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1b38dc0-ad08-4700-a324-fa5eccf3e41e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create a dataset and collect assessments\n",
    "\n",
    "We bootstrap the evaluation dataset using synthetic data generation. For more details on synthetic evals, see [Synthesize Evaluation sets](https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/synthesize-evaluation-set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "497caa8e-5882-4982-8c3f-d39786879570",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate synth evals"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from databricks.agents.evals import generate_evals_df\n",
    "import pandas as pd\n",
    "#TODO Change UC_CATALOG, UC_SCHEMA and DOCT_DATA_TABLE Names\n",
    "docs = spark.sql(f\"SELECT doc_content as content, path as doc_uri FROM {CATALOG}.{SCHEMA}.{DATA_TABLE}\")\n",
    "display(docs)\n",
    "\n",
    "\n",
    "agent_description = \"\"\"\n",
    "The Agent is a RAG chatbot that answers questions about using Ultra Clean tech Products. The Agent has access to a corpus of Ultra tech equipment manuals, and its task is to answer the user's questions by retrieving the relevant docs from the corpus and synthesizing a helpful, accurate response. The corpus covers a lot of info, but the Agent is specifically designed to interact with Support agents who have questions about Ultra Tech equipment from their customers. So questions outside of this scope are considered irrelevant.\n",
    "\"\"\"\n",
    "question_guidelines = \"\"\"\n",
    "# User personas\n",
    "- A Support engineer who is trying to answer questions asked by customers\n",
    "- An experienced, highly technical end customer who might have questions around Fluke equipment\n",
    "\n",
    "# Example questions\n",
    "- What is the use of the HRG regulators?\n",
    "\n",
    "# Additional Guidelines\n",
    "- Questions should be succinct, and human-like\n",
    "\"\"\"\n",
    "\n",
    "num_evals = 10\n",
    "\n",
    "evals = generate_evals_df(\n",
    "    docs,\n",
    "    # The total number of evals to generate. The method attempts to generate evals that have full coverage over the documents\n",
    "    # provided. If this number is less than the number of documents, some documents will not have any evaluations generated. \n",
    "    # For details about how `num_evals` is used to distribute evaluations across the documents, \n",
    "    # see the documentation: \n",
    "    # AWS: https://docs.databricks.com/en/generative-ai/agent-evaluation/synthesize-evaluation-set.html#num-evals. \n",
    "    # Azure: https://learn.microsoft.com/azure/databricks/generative-ai/agent-evaluation/synthesize-evaluation-set \n",
    "    num_evals=num_evals,\n",
    "    # A set of guidelines that help guide the synthetic generation. This is a free-form string that will be used to prompt the generation.\n",
    "    agent_description=agent_description,\n",
    "    question_guidelines=question_guidelines\n",
    ")\n",
    "\n",
    "display(evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b91a20da-43d8-4a31-9661-438b8c3e9171",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "This cell adds the evals above to the evaluation dataset. The evaluation dataset is backed by a Delta table in Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23d4e09d-067d-40d9-b24b-586086ae3039",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add the evals to a dataset and make a labeling session"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from databricks.agents import review_app\n",
    "from databricks.agents import datasets\n",
    "from databricks.sdk.errors import NotFound\n",
    "from IPython.display import Markdown\n",
    "\n",
    "uc_table_name = f\"{CATALOG}.{SCHEMA}.{TABLE}\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "try:\n",
    "    datasets.delete_dataset(uc_table_name)\n",
    "except NotFound:\n",
    "    pass\n",
    "dataset = datasets.create_dataset(uc_table_name)\n",
    "\n",
    "# Add synthetic evals to the dataset.\n",
    "dataset.insert(evals)\n",
    "\n",
    "display(Markdown(f\"Explore dataset in UC: [{uc_table_name}](/explore/data/{CATALOG}/{SCHEMA}/{TABLE}?activeTab=sample)\"))\n",
    "\n",
    "display(spark.read.table(uc_table_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "608c5f11-3272-41b0-ac99-4aac7991d2ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Register an agent with the Review App\n",
    "\n",
    "The following cell adds an agent to the review app for the SME to use in \"chat\" mode or labeling. The Agent will get registered with a name, and is used when the labeling session is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9840e3c-8bc4-4361-8fce-2d4d993fbbd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The review app is tied to the experiment_id set above.\n",
    "my_app = review_app.get_review_app()\n",
    "\n",
    "# Add the llama3 70b model endpoint for labeling. You should replace this with your own model serving endpoint.\n",
    "my_app = my_app.add_agent(\n",
    "    agent_name=\"vs_agent\",\n",
    "    model_serving_endpoint=\"agents_ankit_yadav-uct_schema-vs_agent\",\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19539582-f61d-4e5d-870b-37b4dc28dcbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create a labeling session from the eval dataset\n",
    "\n",
    "The following cell creates a labeling session for the SME to review the dataset we created above.\n",
    "\n",
    "We will configure the labeling session with a set of label schemas, which are the questions that get asked to the SME.\n",
    "\n",
    "We will ask the SME:\n",
    "- \"Please provide a list of facts that you expect to see in a correct response\" and collect a set of \"expected_facts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c0d7369-9c08-4c36-addb-7299f1132da9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optionally remove all previous labeling sessions so this is the only session we ask the SME.\n",
    "# for session in my_app.get_labeling_sessions():\n",
    "#     my_app.delete_labeling_session(session)\n",
    "\n",
    "my_session = my_app.create_labeling_session(\n",
    "    name=\"expected_facts\",\n",
    "    assigned_users=ASSIGNED_USERS,\n",
    "    agent=\"vs_agent\",\n",
    "    # Built-in labeling schemas: EXPECTED_FACTS, GUIDELINES, EXPECTED_RESPONSE\n",
    "    label_schemas=[review_app.label_schemas.EXPECTED_FACTS],\n",
    ")\n",
    "my_session.add_dataset(uc_table_name)\n",
    "\n",
    "# Share with the SME.\n",
    "print(\"Review App URL:\", my_app.url)\n",
    "print(\"Labeling session URL: \", my_session.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef90f93c-aa58-41ab-8df3-d7aac93985a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To see the progress/results of the labeling session. Each row is a trace (an execution of the agent) with associated assessments presented in the \"assessments\" column and under `trace.info.assessments`.\n",
    "mlflow.search_traces(run_id=my_session.mlflow_run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e2f0a63-e257-40fb-957e-29d9d374ed6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Sync _expectations_ back to the evaluation dataset\n",
    "\n",
    "After the SME is done labeling, you can sync the _expectations_ back to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79d4922e-965f-4311-a9d9-b5d21eb16614",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_session.sync_expectations(to_dataset=uc_table_name)\n",
    "display(spark.read.table(uc_table_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1726392b-cfee-4382-aea5-00e2dd766f77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we can run evaluations using the updated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93328326-8402-4024-bfdf-e742ddfd37b2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run mlflow.evaluate on the dataset"
    }
   },
   "outputs": [],
   "source": [
    "global_guidelines = {\n",
    "    \"professional\": [\"The response must be professional.\"],\n",
    "}\n",
    "\n",
    "def my_agent(request):\n",
    "    return \"I'm not feeling great. Don't bother me!\"\n",
    "\n",
    "print(dataset.to_df())\n",
    "\n",
    "eval_results = mlflow.evaluate(\n",
    "    model=my_agent,\n",
    "    data=dataset.to_df(),\n",
    "    model_type=\"databricks-agent\",\n",
    "    evaluator_config={\"databricks-agent\": {\"global_guidelines\": global_guidelines}},\n",
    ")\n",
    "display(eval_results.tables[\"eval_results\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ccf6c4a-39b2-4337-91bd-8f57a5c4bbbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Label traces from an MLFlow run\n",
    "\n",
    "If you already have traces logged into a run, you can add them to the labeling session for your SME to provide assessments. Below we log example traces to an MLFlow run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c860eff9-f5e4-45b8-ba36-b44ec51d666e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "@mlflow.trace(span_type=\"AGENT\")\n",
    "def llama3_agent(messages):\n",
    "  SYSTEM_PROMPT = \"\"\"\n",
    "    You are a chatbot that answers questions about Databricks.\n",
    "    For requests unrelated to Databricks, reject the request.\n",
    "  \"\"\"\n",
    "  return get_deploy_client(\"databricks\").predict(\n",
    "    endpoint=\"agents_ankit_yadav-uct_schema-vs_agent\",\n",
    "    inputs={\"messages\": [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, *messages]}\n",
    "  )\n",
    "\n",
    "# Log example traces to be labeled.\n",
    "with mlflow.start_run(run_name=\"llama3\") as run:\n",
    "    run_id = run.info.run_id\n",
    "    llama3_agent([{\"content\": \"What is databricks?\", \"role\": \"user\"}])\n",
    "    llama3_agent([{\"content\": \"How do I set up a SQL Warehouse?\", \"role\": \"user\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "865f4ada-a08d-4f76-b491-311cbe0e5621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Add the traces to a labeling session\n",
    "\n",
    "Below we select the traces from the run above and add them to a labeling session with a custom _feedback_ label that asks our SME to label the formality of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b224846-2f10-4dbc-8e3c-b85670fe345d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The review app is tied to the current MLFlow experiment.\n",
    "my_app = review_app.get_review_app()\n",
    "\n",
    "# Search for the traces above using the run_id above\n",
    "traces = mlflow.search_traces(run_id=run_id)\n",
    "\n",
    "formality_label_schema = my_app.create_label_schema(\n",
    "  name=\"formal\",\n",
    "  # Type can be \"expectation\" or \"feedback\".\n",
    "  type=\"feedback\",\n",
    "  title=\"Is the response formal?\",\n",
    "  input=review_app.label_schemas.InputCategorical(options=[\"Yes\", \"No\"]),\n",
    "  instruction=\"Please provide a rationale below.\",\n",
    "  enable_comment=True,\n",
    "  overwrite=True\n",
    ")\n",
    "\n",
    "my_session = my_app.create_labeling_session(\n",
    "  name=\"my_session\",\n",
    "  assigned_users=ASSIGNED_USERS,\n",
    "  label_schemas=[\"formal\"]\n",
    ")\n",
    "# NOTE: This will copy the traces into this labeling session so that labels do not modify the original traces.\n",
    "my_session.add_traces(traces)\n",
    "print(my_session.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20cd6488-db6f-428f-b73e-6192149d6223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After the SME is done labeling, we can see the results via `search_traces`, like earlier in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93caae1a-ecb0-4b5c-9fe0-494b744fdee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for row in mlflow.search_traces(run_id=my_session.mlflow_run_id).to_dict(orient=\"records\"):\n",
    "  print(f'{row[\"request_id\"]}: {row[\"assessments\"]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fee8201e-0108-4f30-bbdb-c22da6029f49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Label traces from an Inference table\n",
    "\n",
    "If you already have traces in an inference table (request logs), you can add them to the labeling session for your SME to provide assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24d022b4-045b-4788-b33f-6b719cc04887",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CHANGE TO YOUR PAYLOAD REQUEST LOGS TABLE\n",
    "PAYLOAD_REQUEST_LOGS_TABLE = \"catalog.schema.my_serving_endpoint_payload_request_logs\"\n",
    "traces = spark.table(PAYLOAD_REQUEST_LOGS_TABLE).select(\"trace\").limit(3).toPandas()\n",
    "\n",
    "my_session = my_app.create_labeling_session(\n",
    "  name=\"my_session\",\n",
    "  assigned_users=ASSIGNED_USERS,\n",
    "  label_schemas=[\"formal\"]\n",
    ")\n",
    "\n",
    "# NOTE: This will copy the traces into this labeling session so that labels do not modify the original traces.\n",
    "my_session.add_traces(traces)\n",
    "print(my_session.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7957c300-633f-41bb-b1b3-fbe10177694e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After the SME is done labeling, we can see the results via `search_traces`, like earlier in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3202bab6-fdbc-4917-8297-d2e6a00ec7b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for row in mlflow.search_traces(run_id=my_session.mlflow_run_id).to_dict(orient=\"records\"):\n",
    "  print(f'{row[\"request_id\"]}: {row[\"assessments\"]}\\n')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "review-app",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
