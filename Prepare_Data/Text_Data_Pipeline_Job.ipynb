{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac5f3a80-5842-4182-b96b-9aa3a9755cac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Text Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59682390-8020-4c19-85bf-1bd5b1f750af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook demonstrates an end-to-end pipeline for processing text data using Databricks, focusing on:\n",
    "\n",
    "1. Loading text files (PDF, HTML, DOCX) from a source volume\n",
    "2. Chunking the transcribed text into manageable segments\n",
    "3. Creating a vector search index for efficient retrieval\n",
    "\n",
    "The notebook is structured in sequential steps, from data ingestion through to indexing, making it easy to understand and modify for your specific text processing needs. Each major section is clearly commented and includes relevant configuration parameters.\n",
    "\n",
    "Key components used:\n",
    "- Databricks Vector Search\n",
    "- BGE embedding model for text vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a6bf17a-f6b8-48ac-a895-4ec79c8ed667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Setup and Configuration\n",
    " \n",
    "This section defines key configuration parameters used throughout the notebook:\n",
    " \n",
    " - Unity Catalog settings (catalog, schema, volume names)\n",
    " - Model endpoints (Whisper AI, BGE embeddings) \n",
    " - Delta table names\n",
    " - Vector search configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2c037b7-09b6-4fb6-a23b-da23996deaa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Packages required by all code.\n",
    "# Versions of Databricks code are not locked since Databricks ensures changes are backwards compatible.\n",
    "# Versions of open source packages are locked since package authors often make backwards compatible changes\n",
    "%pip install -qqqq -U \\\n",
    "  databricks-vectorsearch databricks-agents pydantic databricks-sdk mlflow mlflow-skinny `# For agent & data pipeline code` \\\n",
    "  pypdf==4.1.0  `# PDF parsing` \\\n",
    "  markdownify==0.12.1  `# HTML parsing` \\\n",
    "  pypandoc_binary==1.13  `# DOCX parsing` \\\n",
    "  transformers==4.41.1 torch==2.3.0 tiktoken==0.7.0 langchain-text-splitters==0.2.0. `# get_recursive_character_text_splitter`\n",
    "\n",
    "# Restart to load the packages into the Python environment\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa9a5a45-7116-464a-bd73-7c61415e29c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../global_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27d0f730-ee9f-4051-bb51-e6406a74424d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import io\n",
    "import traceback\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# PDF libraries\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# HTML libraries\n",
    "from markdownify import markdownify as md\n",
    "import markdownify\n",
    "import re\n",
    "\n",
    "## DOCX libraries\n",
    "import pypandoc\n",
    "import tempfile\n",
    "\n",
    "# Schema of the dict returned by `file_parser(...)`\n",
    "class ParserReturnValue(TypedDict):\n",
    "    # DO NOT CHANGE THESE NAMES - these are required by Evaluation & Framework\n",
    "    # Parsed content of the document\n",
    "    doc_content: str  # do not change this name\n",
    "    # The status of whether the parser succeeds or fails, used to exclude failed files downstream\n",
    "    parser_status: str  # do not change this name\n",
    "    # Unique ID of the document\n",
    "    doc_uri: str  # do not change this name\n",
    "\n",
    "    # OK TO CHANGE THESE NAMES\n",
    "    # Optionally, you can add additional metadata fields here\n",
    "    example_metadata: str\n",
    "    last_modified: datetime\n",
    "\n",
    "\n",
    "# Parser function.  Replace this function to provide custom parsing logic.\n",
    "def file_parser(\n",
    "    raw_doc_contents_bytes: bytes,\n",
    "    doc_path: str,\n",
    "    modification_time: datetime,\n",
    "    doc_bytes_length: int,\n",
    ") -> ParserReturnValue:\n",
    "    \"\"\"\n",
    "    Parses the content of a PDF document into a string.\n",
    "\n",
    "    This function takes the raw bytes of a PDF document and its path, attempts to parse the document using PyPDF,\n",
    "    and returns the parsed content and the status of the parsing operation.\n",
    "\n",
    "    Parameters:\n",
    "    - raw_doc_contents_bytes (bytes): The raw bytes of the document to be parsed (set by Spark when loading the file)\n",
    "    - doc_path (str): The DBFS path of the document, used to verify the file extension (set by Spark when loading the file)\n",
    "    - modification_time (timestamp): The last modification time of the document (set by Spark when loading the file)\n",
    "    - doc_bytes_length (long): The size of the document in bytes (set by Spark when loading the file)\n",
    "\n",
    "    Returns:\n",
    "    - ParserReturnValue: A dictionary containing the parsed document content and the status of the parsing operation.\n",
    "      The 'doc_content' key will contain the parsed text as a string, and the 'parser_status' key will indicate\n",
    "      whether the parsing was successful or if an error occurred.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        filename, file_extension = os.path.splitext(doc_path)\n",
    "\n",
    "        if file_extension == \".pdf\":\n",
    "            pdf = io.BytesIO(raw_doc_contents_bytes)\n",
    "            reader = PdfReader(pdf)\n",
    "\n",
    "            parsed_content = [\n",
    "                page_content.extract_text() for page_content in reader.pages\n",
    "            ]\n",
    "\n",
    "            parsed_document = {\n",
    "                \"doc_content\": \"\\n\".join(parsed_content),\n",
    "                \"parser_status\": \"SUCCESS\",\n",
    "            }\n",
    "        elif file_extension == \".html\":\n",
    "            from markdownify import markdownify as md\n",
    "\n",
    "            html_content = raw_doc_contents_bytes.decode(\"utf-8\")\n",
    "\n",
    "            markdown_contents = md(\n",
    "                str(html_content).strip(), heading_style=markdownify.ATX\n",
    "            )\n",
    "            markdown_stripped = re.sub(r\"\\n{3,}\", \"\\n\\n\", markdown_contents.strip())\n",
    "\n",
    "            parsed_document = {\n",
    "                \"doc_content\": markdown_stripped,\n",
    "                \"parser_status\": \"SUCCESS\",\n",
    "            }\n",
    "        elif file_extension == \".docx\":\n",
    "            with tempfile.NamedTemporaryFile(delete=True) as temp_file:\n",
    "                temp_file.write(raw_doc_contents_bytes)\n",
    "                temp_file_path = temp_file.name\n",
    "                md = pypandoc.convert_file(temp_file_path, \"markdown\", format=\"docx\")\n",
    "\n",
    "                parsed_document = {\n",
    "                    \"doc_content\": md.strip(),\n",
    "                    \"parser_status\": \"SUCCESS\",\n",
    "                }\n",
    "        else:\n",
    "            raise Exception(f\"No supported parser for {doc_path}\")\n",
    "\n",
    "        # Extract the required doc_uri\n",
    "        # convert from `dbfs:/Volumes/catalog/schema/pdf_docs/filename.pdf` to `Volumes/catalog/schema/pdf_docs/filename.pdf`\n",
    "        modified_path = urlparse(doc_path).path.lstrip('/')\n",
    "        parsed_document[\"doc_uri\"] = modified_path\n",
    "\n",
    "        # Sample metadata extraction logic\n",
    "        if \"test\" in parsed_document[\"doc_content\"]:\n",
    "            parsed_document[\"example_metadata\"] = \"test\"\n",
    "        else:\n",
    "            parsed_document[\"example_metadata\"] = \"not test\"\n",
    "\n",
    "        # Add the modified time\n",
    "        parsed_document[\"last_modified\"] = modification_time\n",
    "\n",
    "        return parsed_document\n",
    "\n",
    "    except Exception as e:\n",
    "        status = f\"An error occurred: {e}\\n{traceback.format_exc()}\"\n",
    "        warnings.warn(status)\n",
    "        return {\n",
    "            \"doc_content\": \"\",\n",
    "            \"parser_status\": f\"ERROR: {status}\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e71b24a-bddb-43f0-b8ab-40cbf88ca156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### `typed_dicts_to_spark_schema`\n",
    "\n",
    "`typed_dicts_to_spark_schema` converts multiple TypedDicts into a Spark schema, allowing for the combination of multiple TypedDicts into a single Spark DataFrame schema. This function enables the resulting Delta Table to reflect the schema defined in `ParserReturnValue`.\n",
    "\n",
    "Arguments:\n",
    "- `*typed_dicts`: A variable number of TypedDict classes to be converted.\n",
    "\n",
    "Returns:\n",
    "- `StructType`: A Spark schema represented as a StructType object, which is a collection of StructField objects derived from the provided TypedDicts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cef17f8-0e00-4838-9074-95b13a67c073",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    DoubleType,\n",
    "    BooleanType,\n",
    "    ArrayType,\n",
    "    TimestampType,\n",
    "    DateType,\n",
    ")\n",
    "from typing import TypedDict, get_type_hints, List\n",
    "from datetime import datetime, date, time\n",
    "\n",
    "\n",
    "def typed_dict_to_spark_fields(typed_dict: type[TypedDict]) -> StructType:\n",
    "    \"\"\"\n",
    "    Converts a TypedDict into a list of Spark StructField objects.\n",
    "\n",
    "    This function maps Python types defined in a TypedDict to their corresponding\n",
    "    Spark SQL data types, facilitating the creation of a Spark DataFrame schema\n",
    "    from Python type annotations.\n",
    "\n",
    "    Parameters:\n",
    "    - typed_dict (type[TypedDict]): The TypedDict class to be converted.\n",
    "\n",
    "    Returns:\n",
    "    - StructType: A list of StructField objects representing the Spark schema.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If an unsupported type is encountered or if dictionary types are used.\n",
    "    \"\"\"\n",
    "\n",
    "    # Mapping of type names to Spark type objects\n",
    "    type_mapping = {\n",
    "        str: StringType(),\n",
    "        int: IntegerType(),\n",
    "        float: DoubleType(),\n",
    "        bool: BooleanType(),\n",
    "        list: ArrayType(StringType()),  # Default to StringType for arrays\n",
    "        datetime: TimestampType(),\n",
    "        date: DateType(),\n",
    "    }\n",
    "\n",
    "    def get_spark_type(value_type):\n",
    "        \"\"\"\n",
    "        Helper function to map a Python type to a Spark SQL data type.\n",
    "\n",
    "        This function supports basic Python types, lists of a single type, and raises\n",
    "        an error for unsupported types or dictionaries.\n",
    "\n",
    "        Parameters:\n",
    "        - value_type: The Python type to be converted.\n",
    "\n",
    "        Returns:\n",
    "        - DataType: The corresponding Spark SQL data type.\n",
    "\n",
    "        Raises:\n",
    "        - ValueError: If the type is unsupported or if dictionary types are used.\n",
    "        \"\"\"\n",
    "        if value_type in type_mapping:\n",
    "            return type_mapping[value_type]\n",
    "        elif hasattr(value_type, \"__origin__\") and value_type.__origin__ == list:\n",
    "            # Handle List[type] types\n",
    "            return ArrayType(get_spark_type(value_type.__args__[0]))\n",
    "        elif hasattr(value_type, \"__origin__\") and value_type.__origin__ == dict:\n",
    "            # Handle Dict[type, type] types (not fully supported)\n",
    "            raise ValueError(\"Dict types are not fully supported\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported type: {value_type}\")\n",
    "\n",
    "    # Get the type hints for the TypedDict\n",
    "    type_hints = get_type_hints(typed_dict)\n",
    "\n",
    "    # Convert the type hints into a list of StructField objects\n",
    "    fields = [\n",
    "        StructField(key, get_spark_type(value), True)\n",
    "        for key, value in type_hints.items()\n",
    "    ]\n",
    "\n",
    "    # Create and return the StructType object\n",
    "    return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c08baf5f-72af-425a-90c2-3288f6ae0111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def typed_dicts_to_spark_schema(*typed_dicts: type[TypedDict]) -> StructType:\n",
    "    \"\"\"\n",
    "    Converts multiple TypedDicts into a Spark schema.\n",
    "\n",
    "    This function allows for the combination of multiple TypedDicts into a single\n",
    "    Spark DataFrame schema, enabling the creation of complex data structures.\n",
    "\n",
    "    Parameters:\n",
    "    - *typed_dicts: Variable number of TypedDict classes to be converted.\n",
    "\n",
    "    Returns:\n",
    "    - StructType: A Spark schema represented as a StructType object, which is a collection\n",
    "      of StructField objects derived from the provided TypedDicts.\n",
    "    \"\"\"\n",
    "    fields = []\n",
    "    for typed_dict in typed_dicts:\n",
    "        fields.extend(typed_dict_to_spark_fields(typed_dict))\n",
    "\n",
    "    return StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a9ad0f3-7324-45f3-8f40-8e6ab7cb8e3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "import tiktoken\n",
    "from typing import Callable, Tuple, Optional\n",
    "import os\n",
    "import re\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Constants\n",
    "HF_CACHE_DIR = \"/tmp/hf_cache/\"\n",
    "\n",
    "# Embedding Models Configuration\n",
    "EMBEDDING_MODELS = {\n",
    "    \"gte-large-en-v1.5\": {\n",
    "        \"tokenizer\": lambda: AutoTokenizer.from_pretrained(\n",
    "            \"Alibaba-NLP/gte-large-en-v1.5\", cache_dir=HF_CACHE_DIR\n",
    "        ),\n",
    "        \"context_window\": 8192,\n",
    "        \"type\": \"SENTENCE_TRANSFORMER\",\n",
    "    },\n",
    "    \"bge-large-en-v1.5\": {\n",
    "        \"tokenizer\": lambda: AutoTokenizer.from_pretrained(\n",
    "            \"BAAI/bge-large-en-v1.5\", cache_dir=HF_CACHE_DIR\n",
    "        ),\n",
    "        \"context_window\": 512,\n",
    "        \"type\": \"SENTENCE_TRANSFORMER\",\n",
    "    },\n",
    "    \"bge_large_en_v1_5\": {\n",
    "        \"tokenizer\": lambda: AutoTokenizer.from_pretrained(\n",
    "            \"BAAI/bge-large-en-v1.5\", cache_dir=HF_CACHE_DIR\n",
    "        ),\n",
    "        \"context_window\": 512,\n",
    "        \"type\": \"SENTENCE_TRANSFORMER\",\n",
    "    },\n",
    "    \"text-embedding-ada-002\": {\n",
    "        \"context_window\": 8192,\n",
    "        \"tokenizer\": lambda: tiktoken.encoding_for_model(\"text-embedding-ada-002\"),\n",
    "        \"type\": \"OPENAI\",\n",
    "    },\n",
    "    \"text-embedding-3-small\": {\n",
    "        \"context_window\": 8192,\n",
    "        \"tokenizer\": lambda: tiktoken.encoding_for_model(\"text-embedding-3-small\"),\n",
    "        \"type\": \"OPENAI\",\n",
    "    },\n",
    "    \"text-embedding-3-large\": {\n",
    "        \"context_window\": 8192,\n",
    "        \"tokenizer\": lambda: tiktoken.encoding_for_model(\"text-embedding-3-large\"),\n",
    "        \"type\": \"OPENAI\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def get_workspace_client() -> WorkspaceClient:\n",
    "    \"\"\"Returns a WorkspaceClient instance.\"\"\"\n",
    "    return WorkspaceClient()\n",
    "\n",
    "\n",
    "def get_embedding_model_config(endpoint_type: str) -> Optional[dict]:\n",
    "    \"\"\"\n",
    "    Retrieve embedding model configuration by endpoint type.\n",
    "    \"\"\"\n",
    "    return EMBEDDING_MODELS.get(endpoint_type)\n",
    "\n",
    "\n",
    "def extract_endpoint_type(llm_endpoint) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract the endpoint type from the given llm_endpoint object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return llm_endpoint.config.served_entities[0].external_model.name\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            return llm_endpoint.config.served_entities[0].foundation_model.name\n",
    "        except AttributeError:\n",
    "            return None\n",
    "\n",
    "\n",
    "def detect_fmapi_embedding_model_type(\n",
    "    model_serving_endpoint: str,\n",
    ") -> Tuple[Optional[str], Optional[dict]]:\n",
    "    \"\"\"\n",
    "    Detects the embedding model type and configuration for the given endpoint.\n",
    "    Returns a tuple of (endpoint_type, embedding_config) or (None, None) if not found.\n",
    "    \"\"\"\n",
    "    client = get_workspace_client()\n",
    "\n",
    "    try:\n",
    "        llm_endpoint = client.serving_endpoints.get(name=model_serving_endpoint)\n",
    "        endpoint_type = extract_endpoint_type(llm_endpoint)\n",
    "    except Exception as e:\n",
    "        endpoint_type = None\n",
    "\n",
    "    embedding_config = (\n",
    "        get_embedding_model_config(endpoint_type) if endpoint_type else None\n",
    "    )\n",
    "    return (endpoint_type, embedding_config)\n",
    "\n",
    "\n",
    "def validate_chunk_size(chunk_spec: dict):\n",
    "    \"\"\"\n",
    "    Validate the chunk size and overlap settings in chunk_spec.\n",
    "    Raises ValueError if any condition is violated.\n",
    "    \"\"\"\n",
    "    if (\n",
    "        chunk_spec[\"chunk_overlap_tokens\"] + chunk_spec[\"chunk_size_tokens\"]\n",
    "    ) > chunk_spec[\"context_window\"]:\n",
    "        raise ValueError(\n",
    "            f'Proposed chunk_size of {chunk_spec[\"chunk_size_tokens\"]} + overlap of {chunk_spec[\"chunk_overlap_tokens\"]} '\n",
    "            f'is {chunk_spec[\"chunk_overlap_tokens\"] + chunk_spec[\"chunk_size_tokens\"]} which is greater than context '\n",
    "            f'window of {chunk_spec[\"context_window\"]} tokens.'\n",
    "        )\n",
    "\n",
    "    if chunk_spec[\"chunk_overlap_tokens\"] > chunk_spec[\"chunk_size_tokens\"]:\n",
    "        raise ValueError(\n",
    "            f'Proposed `chunk_overlap_tokens` of {chunk_spec[\"chunk_overlap_tokens\"]} is greater than the '\n",
    "            f'`chunk_size_tokens` of {chunk_spec[\"chunk_size_tokens\"]}. Reduce the size of `chunk_size_tokens`.'\n",
    "        )\n",
    "\n",
    "\n",
    "def get_recursive_character_text_splitter(\n",
    "    model_serving_endpoint: str,\n",
    "    embedding_model_name: str = None,\n",
    "    chunk_size_tokens: int = None,\n",
    "    chunk_overlap_tokens: int = 0,\n",
    ") -> Callable[[str], list[str]]:\n",
    "    try:\n",
    "        # Detect the embedding model and its configuration\n",
    "        embedding_model_name, chunk_spec = detect_fmapi_embedding_model_type(\n",
    "            model_serving_endpoint\n",
    "        )\n",
    "\n",
    "        if chunk_spec is None or embedding_model_name is None:\n",
    "            # Fall back to using provided embedding_model_name\n",
    "            chunk_spec = EMBEDDING_MODELS.get(embedding_model_name)\n",
    "            if chunk_spec is None:\n",
    "                raise KeyError\n",
    "\n",
    "        # Update chunk specification based on provided parameters\n",
    "        chunk_spec[\"chunk_size_tokens\"] = (\n",
    "            chunk_size_tokens or chunk_spec[\"context_window\"]\n",
    "        )\n",
    "        chunk_spec[\"chunk_overlap_tokens\"] = chunk_overlap_tokens\n",
    "\n",
    "        # Validate chunk size and overlap\n",
    "        validate_chunk_size(chunk_spec)\n",
    "\n",
    "        print(f'Chunk size in tokens: {chunk_spec[\"chunk_size_tokens\"]}')\n",
    "        print(f'Chunk overlap in tokens: {chunk_spec[\"chunk_overlap_tokens\"]}')\n",
    "        context_usage = (\n",
    "            round(\n",
    "                (chunk_spec[\"chunk_size_tokens\"] + chunk_spec[\"chunk_overlap_tokens\"])\n",
    "                / chunk_spec[\"context_window\"],\n",
    "                2,\n",
    "            )\n",
    "            * 100\n",
    "        )\n",
    "        print(\n",
    "            f'Using {context_usage}% of the {chunk_spec[\"context_window\"]} token context window.'\n",
    "        )\n",
    "\n",
    "    except KeyError:\n",
    "        raise ValueError(\n",
    "            f\"Embedding model `{embedding_model_name}` not found. Available models: {EMBEDDING_MODELS.keys()}\"\n",
    "        )\n",
    "\n",
    "    def _recursive_character_text_splitter(text: str) -> list[str]:\n",
    "        tokenizer = chunk_spec[\"tokenizer\"]()\n",
    "        if chunk_spec[\"type\"] == \"SENTENCE_TRANSFORMER\":\n",
    "            splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "                tokenizer,\n",
    "                chunk_size=chunk_spec[\"chunk_size_tokens\"],\n",
    "                chunk_overlap=chunk_spec[\"chunk_overlap_tokens\"],\n",
    "            )\n",
    "        elif chunk_spec[\"type\"] == \"OPENAI\":\n",
    "            splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "                tokenizer.name,\n",
    "                chunk_size=chunk_spec[\"chunk_size_tokens\"],\n",
    "                chunk_overlap=chunk_spec[\"chunk_overlap_tokens\"],\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {chunk_spec['type']}\")\n",
    "        return splitter.split_text(text)\n",
    "\n",
    "    return _recursive_character_text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3872e173-e9e9-488d-aec0-14ac37e80d36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "from datetime import datetime\n",
    "from typing import Any, Callable, TypedDict, Dict\n",
    "import os\n",
    "from IPython.display import display_markdown\n",
    "import warnings\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "\n",
    "\n",
    "def _parse_and_extract(\n",
    "    raw_doc_contents_bytes: bytes,\n",
    "    modification_time: datetime,\n",
    "    doc_bytes_length: int,\n",
    "    doc_path: str,\n",
    "    parse_file_udf: Callable[[[dict, Any]], str],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Parses raw bytes & extract metadata.\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Run the parser\n",
    "        parser_output_dict = parse_file_udf(\n",
    "            raw_doc_contents_bytes=raw_doc_contents_bytes,\n",
    "            doc_path=doc_path,\n",
    "            modification_time=modification_time,\n",
    "            doc_bytes_length=doc_bytes_length,\n",
    "        )\n",
    "\n",
    "        if parser_output_dict.get(\"parser_status\") == \"SUCCESS\":\n",
    "            return parser_output_dict\n",
    "        else:\n",
    "            raise Exception(parser_output_dict.get(\"parser_status\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        status = f\"An error occurred: {e}\\n{traceback.format_exc()}\"\n",
    "        warnings.warn(status)\n",
    "        return {\n",
    "            \"doc_content\": \"\",\n",
    "            \"doc_uri\": \"\",\n",
    "            \"parser_status\": status,\n",
    "        }\n",
    "\n",
    "\n",
    "def _get_parser_udf(\n",
    "    # extract_metadata_udf: Callable[[[dict, Any]], str],\n",
    "    parse_file_udf: Callable[[[dict, Any]], str],\n",
    "    spark_dataframe_schema: StructType,\n",
    "):\n",
    "    \"\"\"Gets the Spark UDF which will parse the files in parallel.\n",
    "\n",
    "    Arguments:\n",
    "      - extract_metadata_udf: A function that takes parsed content and extracts the metadata\n",
    "      - parse_file_udf: A function that takes the raw file and returns the parsed text.\n",
    "      - spark_dataframe_schema: The resulting schema of the document delta table\n",
    "    \"\"\"\n",
    "    # This UDF will load each file, parse the doc, and extract metadata.\n",
    "    parser_udf = func.udf(\n",
    "        lambda raw_doc_contents_bytes, modification_time, doc_bytes_length, doc_path: _parse_and_extract(\n",
    "            raw_doc_contents_bytes,\n",
    "            modification_time,\n",
    "            doc_bytes_length,\n",
    "            doc_path,\n",
    "            parse_file_udf,\n",
    "        ),\n",
    "        returnType=spark_dataframe_schema,\n",
    "    )\n",
    "    return parser_udf\n",
    "\n",
    "def load_files_to_df(\n",
    "    spark: SparkSession,\n",
    "    source_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Load files from a directory into a Spark DataFrame.\n",
    "    Each row in the DataFrame will contain the path, length, and content of the file; for more\n",
    "    details, see https://spark.apache.org/docs/latest/sql-data-sources-binaryFile.html\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(source_path):\n",
    "        raise ValueError(\n",
    "            f\"{source_path} passed to `load_uc_volume_files` does not exist.\"\n",
    "        )\n",
    "\n",
    "    # Load the raw riles\n",
    "    raw_files_df = (\n",
    "        spark.read.format(\"binaryFile\").option(\"recursiveFileLookup\", \"true\")\n",
    "        .load(source_path)\n",
    "    )\n",
    "\n",
    "    # Check that files were present and loaded\n",
    "    if raw_files_df.count() == 0:\n",
    "        raise Exception(f\"`{source_path}` does not contain any files.\")\n",
    "\n",
    "    print(f\"Found {raw_files_df.count()} files in {source_path}.\")\n",
    "    raw_files_df.show()\n",
    "    return raw_files_df\n",
    "\n",
    "\n",
    "def apply_parsing_udf(raw_files_df: DataFrame, parse_file_udf: Callable[[[dict, Any]], str], parsed_df_schema: StructType) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Apply a file-parsing UDF to a DataFrame whose rows correspond to file content/metadata loaded via\n",
    "    https://spark.apache.org/docs/latest/sql-data-sources-binaryFile.html\n",
    "    Returns a DataFrame with the parsed content and metadata.\n",
    "    \"\"\"\n",
    "    print(\"Running parsing & metadata extraction UDF in spark...\")\n",
    "\n",
    "    parser_udf = _get_parser_udf(parse_file_udf, parsed_df_schema)\n",
    "\n",
    "    # Run the parsing\n",
    "    parsed_files_staging_df = raw_files_df.withColumn(\n",
    "        \"parsing\", parser_udf(\"content\", \"modificationTime\", \"length\", \"path\")\n",
    "    ).drop(\"content\")\n",
    "\n",
    "    # Check and warn on any errors\n",
    "    errors_df = parsed_files_staging_df.filter(\n",
    "        func.col(f\"parsing.parser_status\") != \"SUCCESS\"\n",
    "    )\n",
    "\n",
    "    num_errors = errors_df.count()\n",
    "    if num_errors > 0:\n",
    "        display_markdown(\n",
    "            f\"### {num_errors} documents had parse errors. Please review.\", raw=True\n",
    "        )\n",
    "        errors_df.show()\n",
    "\n",
    "        if errors_df.count() == parsed_files_staging_df.count():\n",
    "            raise ValueError(\n",
    "                \"All documents produced an error during parsing. Please review.\"\n",
    "            )\n",
    "\n",
    "    num_empty_content = errors_df.filter(func.col(\"parsing.doc_content\") == \"\").count()\n",
    "    if num_empty_content > 0:\n",
    "        display_markdown(\n",
    "            f\"### {num_errors} documents have no content. Please review.\", raw=True\n",
    "        )\n",
    "        errors_df.show()\n",
    "\n",
    "        if num_empty_content == parsed_files_staging_df.count():\n",
    "            raise ValueError(\"All documents are empty. Please review.\")\n",
    "\n",
    "    # Filter for successfully parsed files\n",
    "    # Change the schema to the resulting schema\n",
    "    resulting_fields = [field.name for field in parsed_df_schema.fields]\n",
    "\n",
    "    parsed_files_df = parsed_files_staging_df.filter(\n",
    "        parsed_files_staging_df.parsing.parser_status == \"SUCCESS\"\n",
    "    )\n",
    "\n",
    "    parsed_files_df.show()\n",
    "    parsed_files_df = parsed_files_df.select(\n",
    "        *[func.col(f\"parsing.{field}\").alias(field) for field in resulting_fields]\n",
    "    )\n",
    "    return parsed_files_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc7b51ca-c843-4c46-aece-b2a68ac68fd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, size, split, cast\n",
    "from pyspark.sql.types import StructType, DoubleType\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def process_text_data(\n",
    "    spark: SparkSession,\n",
    "    source_path: str,\n",
    "    parse_file_udf: Any,\n",
    "    parsed_df_schema: StructType,\n",
    "    output_table: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Process text files and create a standardized table with renamed columns.\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        source_path: Path to the source text files\n",
    "        parse_file_udf: UDF for parsing files\n",
    "        parsed_df_schema: Schema for the parsed DataFrame\n",
    "        output_table: Full name of the output table (catalog.schema.table)\n",
    "    \"\"\"\n",
    "    logger.info(f\"Processing text files from {source_path}\")\n",
    "    \n",
    "    # Load raw files\n",
    "    raw_files_df = load_files_to_df(\n",
    "        spark=spark,\n",
    "        source_path=source_path,\n",
    "    )\n",
    "    \n",
    "    # Apply parsing UDF\n",
    "    parsed_files_df = apply_parsing_udf(\n",
    "        raw_files_df=raw_files_df,\n",
    "        parse_file_udf=parse_file_udf,\n",
    "        parsed_df_schema=parsed_df_schema\n",
    "    )\n",
    "    \n",
    "    # First, create a DataFrame with the word count calculation\n",
    "    word_count_df = parsed_files_df.withColumn(\n",
    "        \"word_count\",\n",
    "        size(split(col(\"doc_content\"), \"\\\\s+\"))\n",
    "    )\n",
    "    \n",
    "    # Now create the final DataFrame with all columns\n",
    "    processed_df = word_count_df.select(\n",
    "        col(\"doc_content\"),  # Keep as is\n",
    "        col(\"parser_status\"),  # Keep as is\n",
    "        col(\"doc_uri\").alias(\"path\"),  # Rename doc_uri to path\n",
    "        col(\"last_modified\").alias(\"modificationTime\"),  # Rename last_modified to modificationTime\n",
    "        lit(\"docs\").alias(\"modality\"),  # Add modality column with value \"docs\"\n",
    "        col(\"word_count\").cast(\"double\").alias(\"length\")  # Cast word count to double and rename to length\n",
    "    )\n",
    "    \n",
    "    # Write to Delta table\n",
    "    processed_df.write.mode(\"overwrite\").option(\n",
    "        \"overwriteSchema\", \"true\"\n",
    "    ).saveAsTable(output_table)\n",
    "    \n",
    "    logger.info(f\"Processed {processed_df.count()} documents and saved to {output_table}\")\n",
    "    \n",
    "    # Display for debugging\n",
    "    processed_df.display()\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "from your_module import load_files_to_df, apply_parsing_udf, typed_dicts_to_spark_schema, ParserReturnValue\n",
    "\n",
    "# Configure the processor\n",
    "process_text_data(\n",
    "    spark=spark,\n",
    "    source_path=f'/Volumes/{UC_CATALOG_NAME}/{UC_SCHEMA_NAME}/{UC_VOLUME_NAME}/text_data/',\n",
    "    parse_file_udf=file_parser,\n",
    "    parsed_df_schema=typed_dicts_to_spark_schema(ParserReturnValue),\n",
    "    output_table=f\"{UC_CATALOG_NAME}.{UC_SCHEMA_NAME}.{DOCS_DATA_TABLE_NAME}\"\n",
    ")\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dadea4cd-d808-441c-a8d3-dbd310c0c5cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "process_text_data(\n",
    "    spark=spark,\n",
    "    source_path=f'/Volumes/{UC_CATALOG_NAME}/{UC_SCHEMA_NAME}/{UC_VOLUME_NAME}/text_data/',\n",
    "    parse_file_udf=file_parser,\n",
    "    parsed_df_schema=typed_dicts_to_spark_schema(ParserReturnValue),\n",
    "    output_table=f\"{UC_CATALOG_NAME}.{UC_SCHEMA_NAME}.{DOCS_DATA_TABLE_NAME}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8720a57c-90be-4179-b580-764f53d5aac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Literal, Optional, Any, Callable\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from pyspark.sql.functions import explode\n",
    "import pyspark.sql.functions as func\n",
    "from typing import Callable\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "import tiktoken\n",
    "from pyspark.sql.types import StructType, StringType, StructField, MapType, ArrayType\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def standardize_timestamp_format(df):\n",
    "    \"\"\"\n",
    "    Standardize timestamp format in a DataFrame.\n",
    "    Converts any timestamp column to a consistent format.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "    Returns:\n",
    "        DataFrame with standardized timestamps\n",
    "    \"\"\"\n",
    "    if \"modificationTime\" in df.columns:\n",
    "        return df.withColumn(\n",
    "            \"modificationTime\",\n",
    "            func.to_timestamp(func.col(\"modificationTime\"))\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def compute_chunks(\n",
    "    docs_table: str,\n",
    "    doc_column: str,\n",
    "    chunk_fn: Callable[[str], list[str]],\n",
    "    propagate_columns: list[str],\n",
    "    chunked_docs_table: str,\n",
    "    modality: str,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Compute chunks from a document table and append them to an existing chunked table.\n",
    "    \n",
    "    Args:\n",
    "        docs_table: Source table containing documents\n",
    "        doc_column: Column name containing the text to chunk\n",
    "        chunk_fn: Function to split text into chunks\n",
    "        propagate_columns: List of columns to propagate from the docs table to chunks table\n",
    "        chunked_docs_table: Target table for storing chunks\n",
    "        modality: Type of content (e.g., 'video', 'audio', 'pdf')\n",
    "    Returns:\n",
    "        str: Name of the chunked table\n",
    "    \"\"\"\n",
    "    logger.info(f\"Computing chunks for `{docs_table}`...\")\n",
    "    \n",
    "    # Initialize Spark session if not already available\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # Read source documents\n",
    "    raw_docs = spark.read.table(docs_table)\n",
    "    \n",
    "    # Check if modality column exists in source table\n",
    "    source_has_modality = \"modality\" in raw_docs.columns\n",
    "    \n",
    "    # Create UDF for chunking\n",
    "    parser_udf = func.udf(\n",
    "        chunk_fn,\n",
    "        returnType=ArrayType(StringType()),\n",
    "    )\n",
    "    \n",
    "    # Process documents into chunks\n",
    "    chunked_array_docs = raw_docs.withColumn(\n",
    "        \"content_chunked\", parser_udf(doc_column)\n",
    "    ).drop(doc_column)\n",
    "    \n",
    "    # Select columns to propagate, excluding modality if it exists\n",
    "    columns_to_propagate = [col for col in propagate_columns if col != \"modality\"]\n",
    "    \n",
    "    chunked_docs = chunked_array_docs.select(\n",
    "        *columns_to_propagate, explode(\"content_chunked\").alias(\"content_chunked\")\n",
    "    )\n",
    "    \n",
    "    # Add chunk_id\n",
    "    chunks_with_ids = chunked_docs.withColumn(\n",
    "        \"chunk_id\", func.md5(func.col(\"content_chunked\"))\n",
    "    )\n",
    "    \n",
    "    # Add modality column if it doesn't exist in source\n",
    "    if not source_has_modality:\n",
    "        chunks_with_ids = chunks_with_ids.withColumn(\"modality\", func.lit(modality))\n",
    "    \n",
    "    # Check if target table exists and get its schema\n",
    "    try:\n",
    "        target_schema = spark.read.table(chunked_docs_table).schema\n",
    "        table_exists = True\n",
    "    except Exception:\n",
    "        table_exists = False\n",
    "        target_schema = None\n",
    "    \n",
    "    if table_exists:\n",
    "        target_has_modality = \"modality\" in [field.name for field in target_schema]\n",
    "        \n",
    "        # If target has modality but source doesn't, add it\n",
    "        if target_has_modality and not source_has_modality:\n",
    "            chunks_with_ids = chunks_with_ids.withColumn(\"modality\", func.lit(modality))\n",
    "    \n",
    "    # Standardize timestamp format\n",
    "    chunks_with_ids = standardize_timestamp_format(chunks_with_ids)\n",
    "    \n",
    "    # Reorder columns for better display\n",
    "    final_columns = [\"chunk_id\", \"content_chunked\"]\n",
    "    if \"modality\" in chunks_with_ids.columns:\n",
    "        final_columns.append(\"modality\")\n",
    "    final_columns.extend(columns_to_propagate)\n",
    "    \n",
    "    chunks_with_ids = chunks_with_ids.select(*final_columns)\n",
    "    \n",
    "    if table_exists:\n",
    "        # Read existing chunks\n",
    "        existing_chunks = spark.read.table(chunked_docs_table)\n",
    "        \n",
    "        # Get existing chunk IDs\n",
    "        existing_ids = existing_chunks.select(\"chunk_id\").distinct()\n",
    "        \n",
    "        # Filter out chunks that already exist\n",
    "        new_chunks = chunks_with_ids.join(\n",
    "            existing_ids,\n",
    "            chunks_with_ids.chunk_id == existing_ids.chunk_id,\n",
    "            \"left_anti\"\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Found {chunks_with_ids.count()} total chunks, {new_chunks.count()} new chunks\")\n",
    "        \n",
    "        # Append only new chunks\n",
    "        if new_chunks.count() > 0:\n",
    "            new_chunks.write.mode(\"append\").saveAsTable(chunked_docs_table)\n",
    "            logger.info(f\"Appended {new_chunks.count()} new chunks to {chunked_docs_table}\")\n",
    "        else:\n",
    "            logger.info(\"No new chunks to append\")\n",
    "    else:\n",
    "        # Create new table if it doesn't exist\n",
    "        chunks_with_ids.write.mode(\"overwrite\").option(\n",
    "            \"overwriteSchema\", \"true\"\n",
    "        ).saveAsTable(chunked_docs_table)\n",
    "        logger.info(f\"Created new table {chunked_docs_table} with {chunks_with_ids.count()} chunks\")\n",
    "    \n",
    "    return chunked_docs_table\n",
    "\n",
    "# Example usage code\n",
    "def process_video_chunks():\n",
    "    \"\"\"\n",
    "    Example function to process video transcripts into chunks\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting video chunk processing...\")\n",
    "    \n",
    "    # Configure the chunker\n",
    "    chunk_fn = get_recursive_character_text_splitter(\n",
    "        model_serving_endpoint=EMBEDDING_MODEL_ENDPOINT,\n",
    "        chunk_size_tokens=384,\n",
    "        chunk_overlap_tokens=128,\n",
    "    )\n",
    "    \n",
    "    # Get source table schema\n",
    "    source_table = f\"{UC_CATALOG_NAME}.{UC_SCHEMA_NAME}.{AUDIO_DATA_TABLE_NAME}\"\n",
    "    source_schema = spark.table(source_table).schema\n",
    "    \n",
    "    # Log source table columns\n",
    "    logger.info(f\"Source table columns: {[field.name for field in source_schema]}\")\n",
    "    \n",
    "    # Get the columns to propagate\n",
    "    # Exclude only the columns we definitely don't want\n",
    "    propagate_columns = [\n",
    "        field.name\n",
    "        for field in source_schema\n",
    "        if field.name not in [\"transcript_text\", \"chunk_count\"]  # Keep name and modality\n",
    "    ]\n",
    "    \n",
    "    logger.info(f\"Propagating columns: {propagate_columns}\")\n",
    "    \n",
    "    # Process chunks\n",
    "    chunked_docs_table = compute_chunks(\n",
    "        docs_table=source_table,\n",
    "        doc_column=\"transcript_text\",\n",
    "        chunk_fn=chunk_fn,\n",
    "        propagate_columns=propagate_columns,\n",
    "        chunked_docs_table=CHUNKED_DOCS_DELTA_TABLE,\n",
    "        modality=\"video\"\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    result_df = spark.read.table(chunked_docs_table)\n",
    "    logger.info(f\"Chunked table schema: {result_df.schema}\")\n",
    "    logger.info(f\"Number of chunks created: {result_df.count()}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# To run the processing:\n",
    "# result_df = process_video_chunks()\n",
    "# display(result_df)\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# Define chunking function\n",
    "def chunk_text(text: str) -> list[str]:\n",
    "    # Your chunking logic here\n",
    "    pass\n",
    "\n",
    "# Compute chunks for video transcripts\n",
    "compute_chunks(\n",
    "    docs_table=\"ankit_yadav.fluke_schema.video_data_text\",\n",
    "    doc_column=\"transcript_text\",\n",
    "    chunk_fn=chunk_text,\n",
    "    propagate_columns=[\"name\", \"path\", \"length\"],\n",
    "    chunked_docs_table=\"ankit_yadav.fluke_schema.content_chunks\",\n",
    "    modality=\"video\"\n",
    ")\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30ab98c3-bea1-41ae-901a-abcdc0cabb5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure the chunker\n",
    "chunk_fn = get_recursive_character_text_splitter(\n",
    "    model_serving_endpoint=EMBEDDING_MODEL_ENDPOINT,\n",
    "    chunk_size_tokens=384,\n",
    "    chunk_overlap_tokens=128,\n",
    ")\n",
    "\n",
    "# Get the columns from the parser except for the doc_content\n",
    "# You can modify this to adjust which fields are propagated from the docs table to the chunks table.\n",
    "propagate_columns = [\n",
    "    field.name\n",
    "    for field in spark.table(f\"{UC_CATALOG_NAME}.{UC_SCHEMA_NAME}.{DOCS_DATA_TABLE_NAME}\").schema.fields\n",
    "    if field.name not in [\"doc_content\",\"parser_status\"]\n",
    "]\n",
    "\n",
    "chunked_docs_table = compute_chunks(\n",
    "    # The source documents table.\n",
    "    docs_table=f\"{UC_CATALOG_NAME}.{UC_SCHEMA_NAME}.{DOCS_DATA_TABLE_NAME}\",\n",
    "    # The column containing the documents to be chunked.\n",
    "    doc_column=\"doc_content\",\n",
    "    # The chunking function that takes a string (document) and returns a list of strings (chunks).\n",
    "    chunk_fn=chunk_fn,\n",
    "    # Choose which columns to propagate from the docs table to chunks table. `doc_uri` column is required we can propagate the original document URL to the Agent's web app.\n",
    "    propagate_columns=propagate_columns,\n",
    "    # By default, the chunked_docs_table will be written to `{docs_table}_chunked`.\n",
    "    chunked_docs_table=f\"{CHUNKED_DOCS_DELTA_TABLE}\",\n",
    "    modality=\"docs\"\n",
    ")\n",
    "\n",
    "display(spark.read.table(chunked_docs_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b4ac7a4-0cec-4bbc-b34e-9e3b95bf12b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict, Dict\n",
    "import io\n",
    "from typing import List, Dict, Any, Tuple, Optional, TypedDict\n",
    "import warnings\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import StructType, StringType, StructField, MapType, ArrayType\n",
    "from mlflow.utils import databricks_utils as du\n",
    "from functools import partial\n",
    "import tiktoken\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def _build_index(\n",
    "    primary_key: str,\n",
    "    embedding_source_column: str,\n",
    "    vector_search_endpoint: str,\n",
    "    chunked_docs_table_name: str,\n",
    "    vectorsearch_index_name: str,\n",
    "    embedding_endpoint_name: str,\n",
    "    force_delete=False,\n",
    "):\n",
    "\n",
    "    # Get the vector search index\n",
    "    vsc = VectorSearchClient(disable_notice=True)\n",
    "\n",
    "    def find_index(endpoint_name, index_name):\n",
    "        all_indexes = vsc.list_indexes(name=vector_search_endpoint).get(\n",
    "            \"vector_indexes\", []\n",
    "        )\n",
    "        return vectorsearch_index_name in map(lambda i: i.get(\"name\"), all_indexes)\n",
    "\n",
    "    if find_index(\n",
    "        endpoint_name=vector_search_endpoint, index_name=vectorsearch_index_name\n",
    "    ):\n",
    "        if force_delete:\n",
    "            vsc.delete_index(\n",
    "                endpoint_name=vector_search_endpoint, index_name=vectorsearch_index_name\n",
    "            )\n",
    "            create_index = True\n",
    "        else:\n",
    "            create_index = False\n",
    "            print(\n",
    "                f\"Syncing index {vectorsearch_index_name}, this can take 15 minutes or much longer if you have a larger number of documents...\"\n",
    "            )\n",
    "\n",
    "            sync_result = vsc.get_index(index_name=vectorsearch_index_name).sync()\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            f'Creating non-existent vector search index for endpoint \"{vector_search_endpoint}\" and index \"{vectorsearch_index_name}\"'\n",
    "        )\n",
    "        create_index = True\n",
    "\n",
    "    if create_index:\n",
    "        print(\n",
    "            f\"Computing document embeddings and Vector Search Index. This can take 15 minutes or much longer if you have a larger number of documents.\"\n",
    "        )\n",
    "\n",
    "        vsc.create_delta_sync_index_and_wait(\n",
    "            endpoint_name=vector_search_endpoint,\n",
    "            index_name=vectorsearch_index_name,\n",
    "            primary_key=primary_key,\n",
    "            source_table_name=chunked_docs_table_name,\n",
    "            pipeline_type=\"TRIGGERED\",\n",
    "            embedding_source_column=embedding_source_column,\n",
    "            embedding_model_endpoint_name=embedding_endpoint_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "add3f9d0-f79e-4aa0-8547-ecd0af580ba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class RetrieverIndexResult(BaseModel):\n",
    "    vector_search_endpoint: str\n",
    "    vector_search_index_name: str\n",
    "    embedding_endpoint_name: str\n",
    "    chunked_docs_table: str\n",
    "\n",
    "\n",
    "def build_retriever_index(\n",
    "    chunked_docs_table: str,\n",
    "    primary_key: str,\n",
    "    embedding_source_column: str,\n",
    "    embedding_endpoint_name: str,\n",
    "    vector_search_endpoint: str,\n",
    "    vector_search_index_name: str,\n",
    "    force_delete_vector_search_endpoint=False,\n",
    ") -> RetrieverIndexResult:\n",
    "\n",
    "    retriever_index_result = RetrieverIndexResult(\n",
    "        vector_search_endpoint=vector_search_endpoint,\n",
    "        vector_search_index_name=vector_search_index_name,\n",
    "        embedding_endpoint_name=embedding_endpoint_name,\n",
    "        chunked_docs_table=chunked_docs_table,\n",
    "    )\n",
    "\n",
    "    # Enable CDC for Vector Search Delta Sync\n",
    "    spark.sql(\n",
    "        f\"ALTER TABLE {chunked_docs_table} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\"\n",
    "    )\n",
    "\n",
    "    print(\"Building embedding index...\")\n",
    "    # Building the index.\n",
    "    _build_index(\n",
    "        primary_key=primary_key,\n",
    "        embedding_source_column=embedding_source_column,\n",
    "        vector_search_endpoint=vector_search_endpoint,\n",
    "        chunked_docs_table_name=chunked_docs_table,\n",
    "        vectorsearch_index_name=vector_search_index_name,\n",
    "        embedding_endpoint_name=embedding_endpoint_name,\n",
    "        force_delete=force_delete_vector_search_endpoint,\n",
    "    )\n",
    "\n",
    "    return retriever_index_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1589b45b-c737-4e1f-8204-e1f663fd21f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "retriever_index_result = build_retriever_index(\n",
    "    # Spark requires `` to escape names with special chars, VS client does not.\n",
    "    chunked_docs_table=CHUNKED_DOCS_DELTA_TABLE.replace(\"`\", \"\"),\n",
    "    primary_key=\"chunk_id\",\n",
    "    embedding_source_column=\"content_chunked\",\n",
    "    vector_search_endpoint=VECTOR_SEARCH_ENDPOINT,\n",
    "    vector_search_index_name=VECTOR_INDEX_NAME,\n",
    "    # Must match the embedding endpoint you used to chunk your documents\n",
    "    embedding_endpoint_name=EMBEDDING_MODEL_ENDPOINT,\n",
    "    # Set to true to re-create the vector search endpoint when re-running.\n",
    "    force_delete_vector_search_endpoint=False,\n",
    ")\n",
    "\n",
    "print(retriever_index_result)\n",
    "\n",
    "print()\n",
    "print(\"Vector search index created! This will be used in the next notebook.\")\n",
    "print(f\"Vector search endpoint: {retriever_index_result.vector_search_endpoint}\")\n",
    "print(f\"Vector search index: {retriever_index_result.vector_search_index_name}\")\n",
    "print(f\"Embedding used: {retriever_index_result.embedding_endpoint_name}\")\n",
    "print(f\"Chunked docs table: {retriever_index_result.chunked_docs_table}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Text_Data_Pipeline_Job",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
